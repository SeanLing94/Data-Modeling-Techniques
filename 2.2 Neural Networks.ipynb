{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the neural networks algorithm, we use the mini-case study customer churn dataset that we have previously pre-processed and saved in the ChurnFinal.csv file.  We use the following Python codes to import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Loading Dataset\n",
    "data = pd.read_csv('ChurnFinal.csv')\n",
    "\n",
    "df_inputs = pd.get_dummies(data[['Gender', 'Age', 'PostalCode', 'Cash', 'CreditCard', \n",
    "        'Cheque', 'SinceLastTrx', 'SqrtTotal', 'SqrtMax', 'SqrtMin']])\n",
    "df_label = data['Churn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our problem and data understanding for the customer churn case study (refer to Week 2 content), the solution requires an approach to classify if a customer is going to churn. Thus, we indicate the target attribute as Churn in the data set. We use Gender, Age, PostalCode, Cash, CreditCard, Cheque, SinceLastTrx, SqrtTotal, SqrtMax, and SqrtMin attributes as inputs to predict the churn by assigning them to user-defined variables df_label and df_inputs respectively for later use.\n",
    "\n",
    "Next, we spit the original data set to train and test the model, using the Python train_test_split () function with the following example codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(df_inputs, df_label, \n",
    "                    stratify=df_label, test_size=0.3, random_state=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above codes, we split 80% of the data set for training the model and 20% (i.e., test_size=0.2) as a test set to assess the model.  The random state (random_state) is a seed to the random number generator to ensure numbers are generated in the same order. \n",
    "\n",
    "For a detailed explanation of the train_test_split() API parameters, refer to the official website https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train_test_split#sklearn.model_selection.train_test_split.\n",
    "\n",
    "After splitting the data set into train and test sets, we define the neural network algorithm as the data modeling technique using MLPClassifier(), and train the decision tree using fit() function the train data set (X_train and Y_train) in  function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;identity&#x27;, alpha=1e-05, hidden_layer_sizes=(200, 6),\n",
       "              learning_rate=&#x27;adaptive&#x27;, learning_rate_init=0.0001, max_iter=500,\n",
       "              random_state=1, solver=&#x27;lbfgs&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;identity&#x27;, alpha=1e-05, hidden_layer_sizes=(200, 6),\n",
       "              learning_rate=&#x27;adaptive&#x27;, learning_rate_init=0.0001, max_iter=500,\n",
       "              random_state=1, solver=&#x27;lbfgs&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='identity', alpha=1e-05, hidden_layer_sizes=(200, 6),\n",
       "              learning_rate='adaptive', learning_rate_init=0.0001, max_iter=500,\n",
       "              random_state=1, solver='lbfgs')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)                     #fit only on training data  \n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)       # apply same transformation to test data\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "nn = MLPClassifier(solver='lbfgs', alpha=1e-05 , activation = 'identity', random_state=1,\n",
    "             hidden_layer_sizes=(200,6), learning_rate = 'adaptive', \n",
    "             learning_rate_init=0.0001, max_iter=500)\n",
    "\n",
    "nn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, StandardScaler() function standardizes the data values into a standard format. StandardScaler standardizes an attribute by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation. Further, we use fit_transform() along with the assigned object to transform and standardize the data. StandardScaler() further removes outliers (if they still exist in the data set after we pre-processed the data) since it involves the estimation of the empirical mean and standard deviation of each attribute.\n",
    "\n",
    "For a detailed explanation of the MLPClassifier() API parameters, refer to the official website https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html \n",
    "\n",
    "\n",
    "To assess how well the decision tree model developed based on the criteria above, we use the test data (X_test) for the model to predict the churn outcomes using the predict() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy     :  0.7023\n"
     ]
    }
   ],
   "source": [
    "#Predict the response for test dataset\n",
    "y_predict = nn.predict(X_test)    \n",
    "\n",
    "from sklearn import metrics \n",
    "print(\"Model Accuracy     : \", round(metrics.accuracy_score(Y_test, y_predict),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the how well the classification outcomes, we can derive the model accuracy with the accuracy_score() function by comparing the predicted (i.e., y_predict) and actual (i.e., Y_test) churn outcomes. \n",
    "\n",
    "After running all the codes together given above, we obtain a model accuracy of 0.7023 printed on the console terminal as Moddel Accuracy :     0.7023.  This result indicates that the model has an accuracy of 70.23%, implying that 70.23% of the time it correctly classifies churn outcomes ('yes' or 'no')."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
